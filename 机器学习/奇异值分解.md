# 奇异值分解 Singular Value Decomposition

奇异值分解(Singular Value Decomposition，以下简称SVD)是在机器学习领域广泛应用的算法，它不光可以用于降维算法中的特征分解，还可以用于推荐系统，以及自然语言处理等领域。是很多机器学习算法的基石。本文就对SVD的原理做一个总结，并讨论在在PCA降维算法中是如何运用运用SVD的。

## $1$ 回顾特征值和特征向量

我们首先回顾下特征值和特征向量的定义如下：
$$Ax=\lambda x$$

其中 $A$　是一个 $n\times n$ 的矩阵，$x$ 是一个 $n$ 维向量，则我们说 $λ$ 是矩阵 $A$ 的一个特征值，而 $x$ 是矩阵 $A$ 的特征值 $λ$ 所对应的特征向量。

求出特征值和特征向量有什么好处呢？就是我们可以将矩阵 $A$ 特征分解。如果我们求出了矩阵 $A$ 的 $n$ 个特征值 $\lambda_1\leq \lambda_2\leq\dots≤\lambda_n$,以及这 $n$个特征值所对应的特征向量 $\{w_1,w_2,\dots,w_n\}$，如果这 $n$ 个特征向量线性无关，那么矩阵 $A$ 就可以用下式的特征分解表示：

$$A=W\Sigma W^{−1}$$

其中 $W$ 是这 $n$ 个特征向量所张成的 $n\times n$ 维矩阵，而 $\Sigma$ 为这 $n$ 个特征值为主对角线的 $n\times n$ 维矩阵。

一般我们会把 $W$ 的这 $n$ 个特征向量标准化，即满足 $\|w_i\|_2=1$, 或者说wTiwi=1，此时W的n个特征向量为标准正交基，满足WTW=I，即WT=W−1, 也就是说W为酉矩阵。

　　　　这样我们的特征分解表达式可以写成
A=WΣWT
　　　　注意到要进行特征分解，矩阵A必须为方阵。那么如果A不是方阵，即行和列不相同时，我们还可以对矩阵进行分解吗？答案是可以，此时我们的SVD登场了。

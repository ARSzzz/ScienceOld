# 广义线性模型  Generalized Linear Models

以下为解决线性模型的一系列方法，这些模型的因变量为自变量的线性结合。以 $\hat{y}$为因变量[预测值]，公式为：

$$\hat{y}(\omega,x)=\omega_0+\omega_1 x_1+\cdots+\omega_p x_p
$$

其中，向量 $\omega = (\omega_1,⋯,\omega_p)$ 为参数，$ω_0$ 为截距。

[SKLEARN - 主题页面](http://scikit-learn.org/stable/supervised_learning.html)

## 普通最小二乘法 Ordinary Least Squares

最小二乘法（又称最小平方法）是一种数学优化技术。它通过最小化误差的平方和寻找数据的最佳函数匹配。利用最小二乘法可以简便地求得未知的数据，并使得这些求得的数据与实际数据之间**残差平方和**[residual sum of squares (RSS)]为最小。最小二乘法还可用于曲线拟合。其他一些优化问题也可通过最小化能量或最大化熵用最小二乘法来表达。

数学问题表述为：

$$\min_{w}||Xw-y||_2^2$$

## 岭回归 Ridge Regression

岭回归是一种专用于共线性数据分析的有偏估计回归方法，实质上是一种改良的最小二乘估计法，通过放弃最小二乘法的无偏性，以损失部分信息、降低精度为代价获得回归系数更为符合实际、更可靠的回归方法，对病态数据的拟合要强于最小二乘法。

## Lasso 模型

`Lasso`（`Least absolute shrinkage and selection operator`）是一种回归分析方法，它能同时实现变量选择和正则化，因此统计模型的准确性和可解释性都会提高。

Lasso 通过构造一个惩罚函数得到一个较为精炼的模型，使得它压缩一些系数，同时设定一些系数为零。因此保留了子集收缩的优点，是一种处理具有多重共线性数据的有偏估计。

> 给定如下数据集 $D$：
>
> $$D=\{(x_1, y_1), (x_2, y_2), \dots, (x_n, y_n)\}, \quad
> x \in \R^d, y \in \R \tag{1}$$
>
> 考虑使用最简单的线性回归模型 $y=w^\mathrm{T}x+b$ 来拟合数据以平方误差为损失函数，则优化的目标为 $\min_w\sum_{i=1}^n(y_i - w^\mathrm{T}x_i)^2$。如果引入 $L_1$ 范数正则化项，则优化的目标变为：
>
> $$\min_w\sum_{i=1}^n(y_i - w^\mathrm{T}x_i)^2 + \lambda \cdot ||w||_1 \tag{2}, \quad \lambda > 0$$
>
>其中 $\lambda$ 是 正则化参数。称式 $(2)$ 为 `LASSO`（`Least absolute shrinkage and selection operator`）[Robert Tibshirani, 1996]。

$$\frac 1 {2 \cdot n\_samples} \cdot ||y - Xw||^2_2 + \lambda \cdot ||w||_1$$

- 可以降低过拟合分险。
- 获得的解更稀疏（`sparse`），即最终求得的 $w$ 会有更少的非零分量。

## Elastic Net 回归

Elastic Net是Lasso和Ridge回归技术的混合体。它使用L1来训练并且L2优先作为正则化矩阵。当有多个相关的特征时，Elastic Net是很有用的。Lasso 会随机挑选他们其中的一个，而Elastic Net则会选择两个。

## 最小角回归Least Angle Regression - LAR

最小角回归是一种变量选择的方法，类似于向前逐步回归(Forward Stepwise)的形式。从解的过程上来看它是lasso regression的一种高效解法。

向前逐步回归(Forward Stepwise)不同点在于，Forward Stepwise每次都是根据选择的变量子集，完全拟合出线性模型，计算出RSS，再设计统计量（如AIC）对较高的模型复杂度作出惩罚，而LAR是每次先找出和因变量相关度最高的那个变量, 再沿着LSE的方向一点点调整这个predictor的系数，在这个过程中，这个变量和残差的相关系数会逐渐减小，等到这个相关性没那么显著的时候，就要选进新的相关性最高的变量，然后重新沿着最小二乘LSE的方向进行变动。而到最后，所有变量都被选中，就和最小二乘LSE相同了。

## 逻辑回归 Logistic regression

logistic回归是一种广义线性回归（generalized linear model），因此与多重线性回归分析有很多相同之处。它们的模型形式基本上相同，都具有 $\omega'x+b$，其中 $w$ 和 $b$ 是待求参数，其区别在于他们的因变量不同，多重线性回归直接将 $\omega'x+b$ 作为因变量，即 $y=\omega'x+b$，而logistic回归则通过函数 $L$ 将 $\omega'x+b$ 对应一个隐状态 $p$， $p=L(w' x+b)$，然后根据 $p$ 与 $1-p$ 的大小决定因变量的值。如果 $L$ 是logistic函数，就是logistic回归，如果 $L$ 是多项式函数就是多项式回归。

logistic回归的因变量可以是二分类的，也可以是多分类的，但是二分类的更为常用，也更加容易解释，多类可以使用softmax方法进行处理。实际中最为常用的就是二分类的logistic回归。

## 随机梯度下降 Stochastic Gradient Descent - SGD

缺。

## 感知机 Perceptron

缺。

## 稳健回归 Robustness regression

稳健回归是统计学稳健估计中的一种方法，其主要思路是将对异常值十分敏感的经典最小二乘回归中的目标函数进行修改。经典最小二乘回归以使误差平方和达到最小为其目标函数。因为方差为一不稳健统计量，故最小二乘回归是一种不稳健的方法。不同的目标函数定义了不同的稳健回归方法。

常见的稳健回归方法有：最小中位平方(least median square；LMS)法、M估计法等。

## 多项式回归 Polynomial Regression

研究一个因变量与一个或多个自变量间多项式（自变量的指数大于1）的回归分析方法，称为多项式回归。

多项式回归问题可以通过变量转换化为多元线性回归问题来解决，这是线性模型的扩展运用。

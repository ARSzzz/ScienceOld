# 基于划分的方法 Clustering of partitioning method

## 一、K-均值算法 K-means algorithm

### 1.1 概述 Overview

K-means 算法是典型的基于原型的目标函数聚类方法的代表，它是数据点到原型的某种距离 $\mathrm{dist}(\cdot)$ 作为优化的目标函数，利用函数求极值的方法得到迭代运算的调整规则。

该方法是基于**形心**的聚类算法，使用簇 $C_i$ 的形心 $c_i$ 代表该簇。在这里，形心为分配给该簇的样本的均值。簇 $C_i$ 的质量可以用簇内变差来度量，它是簇 $C_i$ 中所有对象与簇的形心 $c_i$ 之间的误差平方和。数据集中所有对象的误差平方和为：

$$\mathrm{E}=\sum_{i=1}^{k}\sum_{p \in C_i}dist(p,c_i)^2$$

### 1.1 伪代码 Pseudocode

$$\begin{aligned}
    \text{--------------------} &
    \text{--------------------------------------------------------------------------------}\\
    \text{输入：} \\
         &\ \text{样本集：$D=\{x_1, x_2, \dots x_n\}$} \\
         &\ \text{目标簇数：k} \\
    \text{过程：} &\ \text{k\_means(D, k)} \\
    1.\  &\ \text{从 D 中随机选择 k 个样本} \\
    2.\  &\ \text{以该 k 个样本作为初始均值向量 $\Mu =\{\mu_1, \mu_2, \dots, \mu_k\}$} \\
    3.\  &\ \text{while \ True} \\
    4.\  &\ \qquad \text{定义 k 个空集 C$=\{C_1, C_2, \dots, C_k\}$} \\
    5.\  &\ \qquad \text{定义 flag = True} \\
    6.\  &\ \qquad \text{for $x_i$ $in$ D} \\
    7.\  &\ \qquad \qquad \text{for $\mu_j$ $in$ $\Mu$} \\
    8.\  &\ \qquad \qquad \qquad \text{求值 $d_{ij}$ = dist($x_i$,\ $\mu_j$)} \\
    9.\  &\ \qquad \qquad \text{找出 $d_{ij^*}=\min(d_{ij})$} \\
    10.\ &\ \qquad \qquad \text{将 $x_i$ 划入簇 $C_{j^*}$：$C_{j^*}=C_{j^*}\bigcup\{x_i\}$} \\
    11.\ &\ \qquad \text{for $C_i$ $in$ C} \\
    12.\ &\ \qquad \qquad \text{计算新的均值向量：}\mu_i'=\frac{1}{|C_i|}\sum_{x\in C_i}x \\
    13.\ &\ \qquad \qquad \text{if $\mu_i \neq \mu_i'$} \\
    14.\ &\ \qquad \qquad \qquad \text{flag = $False$} \\
    15.\ &\ \qquad \qquad \qquad \text{更新均值向量：$\mu_i=\mu_i'$} \\
    16.\ &\ \qquad \text{if flag $\neq$ $False$} \\
    17.\ &\ \qquad \qquad \text{$break$} \\
    18.\ &\ \text{return C} \\
    \text{--------------------} &
    \text{--------------------------------------------------------------------------------}\\
\end{aligned}$$

编程接口：`sklearn.cluster.KMeans`

### 1.3 算法缺点

- 产生类的大小相差不会很大
- 对于脏数据很敏感。

## 二、K-中心算法 K-medoids algorithm

### 2.1 概述 Overview

K-medoids 是基于代表对象的聚类方法。相较于K-均值，该方法使用簇 $C_i$ 中实际的对象 $o_i$ 来代表该簇。该对象 $o_i$ 为簇的代表对象。

确切的说，是对K-means算法的一种改进算法。

### 2.2 伪代码 Pseudocode

$$\begin{aligned}
    \text{--------------------} &
    \text{--------------------------------------------------------------------------------}\\
    \text{输入：} \\
         &\ \text{样本集：D$=\{x_1, x_2, \dots x_n\}$} \\
         &\ \text{目标簇数：k} \\
    \text{过程：} &\ \text{k\_medoids(D, k)}\\
    1.\  &\ \text{从 D 中随机选择 k 个样本} \\
    2.\  &\ \text{以该 $k$ 个样本作为初始中心 O$=\{o_1, o_2, \dots, o_k\}$} \\
    3.\  &\ \text{$while$ \ True} \\
    4.\  &\ \qquad \text{定义 k 个空集 $C=\{C_1, C_2, \dots, C_k\}$} \\
    5.\  &\ \qquad \text{定义 flag = True} \\
    6.\  &\ \qquad \text{for $x_i$ $in$ D} \\
    7.\  &\ \qquad \qquad \text{for $o_j$ $in$ O}\\
    8.\  &\ \qquad \qquad \qquad \text{求值 $d_{ij}$ = dist($x_i$,\ $o_j$)} \\
    9.\  &\ \qquad \qquad \text{找出 $d_{ij^*}=\min(d_{ij})$} \\
    10.\ &\ \qquad \qquad \text{将 $x_i$ 划入簇 $C_{j^*}$：$C_{j^*}=C_{j^*}\bigcup\{x_i\}$} \\
    11.\ &\ \qquad \text{for $C_j$ $in$ C} \\
    12.\ &\ \qquad \qquad \text{计算 } x_j^* = \min_{x_j \in C_j} \left( \sum_{x_k \in C_j} \mathrm{dist}(x_j, x_k)\right) \\
    13.\ &\ \qquad \qquad \text{if $o_j \neq x_j^*$} \\
    14.\ &\ \qquad \qquad \qquad \text{flag = $False$} \\
    15.\ &\ \qquad \qquad \qquad \text{更新均值向量：$o_j \neq x_j^*$} \\
    16.\ &\ \qquad \text{if flag $\neq$ $False$} \\
    17.\ &\ \qquad \qquad \text{$break$} \\
    18.\ &\ \text{return C} \\
    \text{--------------------} &
    \text{--------------------------------------------------------------------------------}\\
\end{aligned}$$

### 2.3 优缺点

- **优点**：

  - K-mediods算法具有能够处理大型数据集，结果簇相当紧凑，并且簇与簇之间明显分明的优点。
  - 与K-means相比，算法对于噪声不那么敏感。因此离群点不会造成划分的结果偏差过大，少数数据不会造成重大影响。

- **缺点**：
  
  - 必须事先确定类簇数和中心点，簇数和中心点的选择对结果影响很大
  - 一般在获得一个局部最优的解后就停止了
  - 对于除数值型以外的数据不适合
  - 只适用于聚类结果为凸形的数据集等。
  - 由于按照中心点选择的方式进行计算，算法的时间复杂度也比K-means上升了O(n)。

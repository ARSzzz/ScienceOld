<!-- @import "../../引用/my-style.less" -->

# 基本分类算法 Basic classification algorithm

## 〇：基本概念 Basic concepts

> **定义：分类 Classification**
> 数据集 $D$ 是训练元组及其类标号的集合。
> 一个元组表示为一个 $n$ 维向量 $\{x_1,x_2,\dots,x_n\}$，它描述元组的 $n$ 个属性 $\{A_1,A_2,\dots,A_n \}$ 的测量。元组所属类别的所有可能构成了**类集**，每一个类别用类标号表示 $\{C_1,C_2,\cdots,C_m\}$。
> 通过分类算法得到一个分类模型（或称目标函数），它能够根据一个元组的属性 $\{A_1,A_2,\dots,A_n \}$ 的测量来准确判断该元组所属的类别。

许多的分类模型都使用一种**学习算法**（Learning algorithm）来生成。使用学习算法来生成模型，使其拥有很高的**拟合能力**和**泛化能力**。

- 拟合能力：模型能很好地拟合输入数据。
- 泛化能力：能准确预测未知数据属性集的类标号。

## 一、朴素贝叶斯分类器 Naive bayes classifier

贝叶斯分类法基于贝叶斯定理。因为贝叶斯方法的计算过于复杂，故衍生了朴素贝叶斯方法。**朴素**，就是假定**类条件独立**，即**假定一个属性值在给定类上的影响独立于其他的属性值**。

> **算法：贝叶素分类法 Bayes classifier**
> 设 $D$ 是训练元组及其类标号的集合。每个元组表示为一个 $n$ 维向量 $\{x_1,x_2,\cdots,x_n\}$，其描述元组的 $n$ 个属性 $\{A_1,A_2,\cdots,A_n \}$ 的测量。
>设元组有m个类 $\{C_1,C_2,\cdots,C_m\}$。给定元组 $X=\{x_1,x_2,\cdots,x_n\}$，对于任意一个类 $C_i$，其有在条件 $X$ 下该类的后验概率：
> $$P(C_i|X) = \frac {P(C_i )P(X|C_i)} {P(X)},
C_i \in \{C_1,C_2,\cdots,C_m \}$$
>
>比较所有类的上述后验概率，将元组 $X$分配给**具有最高后验概率的类**。

由于 $P(X)$ 对所有类而言为常数，即其大小不影响最终类的选择，故只需要比较各个类的 $P(C_i)P(X|C_i)$ 的大小。

类的先验概率 $P(C_i)$ 可以根据其在训练集合中的频率来估计，或者是假定这些类是等概率的，$P(C_1 )=P(C_2 )=\cdots=P(C_m)$。

如果元组 $X$ 的属性集特别长，计算 $P(X|C_i)$ 的开销可能非常大。为降低计算开销，可以做类条件独立的朴素假定。

> 定义：类条件独立假定
假定属性值有条件的相互独立，即属性之间不存在依赖关系：

$$P(X|C_i) = \prod_{k=1}^nP(x_k|C_i)$$

## 二、贝叶斯信念网络 Bayesian belief networks, BBN

**贝叶斯信念网络**简称**贝叶斯网络**，用图形表示一组随机变量之间的概率关系。

贝叶斯网络有两个主要组成部分：

- **有向无环图**（`dag`），表示变量之间的依赖关系。
- **概率表**，各结点与其直接父结点的概率关系。

**注意**：*贝叶斯网络中的任何结点，如果它的父母结点已知，则它条件独立于它的所有非后代结点。*

如下图1中间的有向无环图中，给定结点 $\mathrm{E}$、结点 $\mathrm{F}$ 条件独立于结点 $\mathrm{D}$ 和 $\mathrm{G}$，因为结点 $\mathrm{D}$ 和 $\mathrm{G}$ 都是 $\mathrm{F}$ 的非后代结点。朴素贝叶斯分类器中的条件独立假设也可以用贝叶斯网络来表示，如图1右边的有向无环图。

## 三、 决策树 Decision tree

### 1.1 基本概念

决策树算法，又称迭代的二分器（Iterative Dichotomiser 3, ID3）。其后，决策树又有后继的具体实现算法C4.5、CART（Classification and Regression Trees）。三种算法都采用贪心方法，即不可回溯。决策树以自顶向下递归的分治方式构造。

#### 1.2.1 伪代码 Pseudocode

> **算法：ID3 决策树**
> **输入**：
>   1. **训练集**：$D$
>   2. **属性集**：$A=\{A_1, A_2, \dots\}$
>
> **过程**：函数 $\text{tree\_generate}(D, A)$
> 1. 建立根结点 node
> 2. 计算 $D$ 中的类集 $C=\{C_1, C_2, \dots\}$
> 3. `if` $D$ 中样本只有一类 $C=\{C_1\}$
>     - 将 node 归为叶结点
>     - node 的类别标记为 $C_1$
>     - `return` node
> 4. `if` $A=\varnothing$ `or` 样本在各个属性 $\{A_1, A_2\}$ 上的取值均相同
>     - 将 node 归为叶结点
>     - node 的类别标记为 $D$ 中样本最多的类
>     - `return` node
> 5. 从 $A$ 中选择最优的划分属性 $a_*$
> 6. `for` $a_*^i$ `in` $a_*$
>     - 为 node 生成一个分支
>     - 令 $D^i$ 为 $D$ 中在属性 $a_*$ 上取值为 $a_*^i$ 的样本
>     - `if` $a_*^i = \varnothing$
>         - 将分支标记为叶结点
>         - 分支的类别标记为 $D$ 中样本最多的类
>     - `else`
>         - 以 $\text{tree\_generate}(D^i,A-a_*)$ 为分支结点
> 7. `return` node

决策树种类的不同，来源于如何从 $A$ 中选择最优的划分属性 $a_*$，也即属性选择度量。

属性选择度量是一种选择分裂准则，把给定类标记的训练元组的数据分区 $D$ “最好地”划分成单独类的启发式方法。

对于给定训练集 $D$ 的标签构成，求出其以2为底数的香农熵 $\Eta(D)$。将 $\Eta(D)$ 作为 $D$ 信息量的衡量，它代表了对 $D$ 中的元组分类所需要的期望信息。

- ID3：`Information gain`（基于 `Entropy` 或 `Gini impurity`）。
- C4.5：`Information gain ratio`。
- CART：`Information gain`。
- CHAID（CHi-squared Automatic Interaction Detector）
- MARS（multivariate adaptive regression splines）

**连续值属性的分裂点选择**：假设上述属性A不是离散的，而是连续的，则选择最佳的分裂点是一个必须要考虑的问题。考虑将属性A分裂为两段的情况。

1. 将 $A$ 的值按增序排列，每对相邻值的中点可以是可能的分裂点。故给定 $A$ 的 $v$ 个可能取值，需要计算 $v-1$ 个可能的划分。
2. 选择最有最小期望信息熵的中点作为分裂点。

## 四、支持向量机 Support vectir machine

略。

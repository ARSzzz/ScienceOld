<!-- @import "../../引用/my-style.less" -->

# 参数估计 Parameter estimation

## 一、点估计

略。

## 二、矩估计

略。

## 三、MLE 算法与 EM 算法 MLE algorithm and EM algorithm

### 3.1 最大似然估计 Maximum likelihood estimation

现在已经拿到了很多个样本（你的数据集中所有因变量），这些样本值已经实现，最大似然估计就是去找到那个（组）参数估计值，使得前面已经实现的样本值发生概率最大。因为你手头上的样本已经实现了，其发生概率最大才符合逻辑。这时是求样本所有观测的联合概率最大化，是个连乘积，只要取对数，就变成了线性加总。此时通过对参数求导数，并令一阶导数为零，就可以通过解方程（组），得到最大似然估计值。

> **定义：似然函数与最大似然估计 Likelihood function and maximum likelihood estimation**
> 设总体的概率函数为 $p(x; \theta), \theta \in \Theta$，其中 $\theta$ 是一个参数或几个参数组成的参数向量，$\Theta$ 是参数空间，$\{x_1, x_2, \cdots, x_n\}$ 是来自该总体的样本。
> 将样本的联合概率函数看成是 $\theta$ 的函数，称为样本的**似然函数**（Likelihood function），用 $\mathrm{L}(\theta;\ x_1, x_2, \dots, x_n)$ 表示，简记为 $\mathrm{L}(\theta)$。
> $$\begin{aligned}
> \mathrm{L}(\theta)
> &=\mathrm{L}(\theta;\ x_1, x_2, \dots, x_n)\\
> &=p(x_1;\ \theta)p(x_2;\ \theta)\dots p(x_n;\ \theta) \\
> &=\prod_{i=1}^n p(x_i;\ \theta)
> \end{aligned}\tag{1}$$
>
> 如果某统计量 $\hat{\theta}=\hat{\theta}(x_1, x_2, \dots, x_n)$ 满足式 $(2)$，则称 $\hat{\theta}$ 是 $\theta$ 的**最大似然估计**（Maximum likelihood estimation），简记为 $\mathrm{MLE}$。
> $$\mathrm{L}(\hat{\theta})=\max_{\theta \in \Theta}\mathrm{L}(\theta)$$

虽然求导函数是求最大似然估计最常用的方法，但并不是在所有场合都有效。要使 $\mathrm{L}(\theta)$ 达到最大，应该：

- 示性函数的取值应该为 $1$。
- 值 $\frac{1}{\theta^n}$ 要尽可能大。

> 性质：不变性
> 如果 $\hat{\theta}$ 是 $\theta$ 的最大似然估计，则对任一函数 $\mathrm{g}(\theta)$，其最大似然估计为 $\mathrm{g}(\hat{\theta})$。

### 3.2 EM 算法 Expectation maximization algorithm

MLE 是一种非常有效的参数估计方法。但有些情况下 MLE 的求取是比较困难的，比如分布中有多余参数，或数据为截尾或缺失时。Dempster 等人在1977年提出了 EM 算法，其出发点是把求 MLE 的过程分两步走：

1. 求出期望，以便把多余的部分去掉。
2. 救出极大值。

#### 3.2.1 算法概述 Algorithm overview

设某随机事件 $A$ 有四种可能的结果 $\{A_1, A_2, A_3, A_4\}$，则概率分布列如下：
$$\begin{matrix}
    A_1                & A_2                & A_3                & A_4 \\
    \frac{2-\theta}{4} & \frac{1-\theta}{4} & \frac{1+\theta}{4} & \frac{\theta}{4}
\end{matrix} \quad,\ \theta \in (0,1)
\tag{1}
$$

现对事件 $A$ 进行若干次试验，以 $\{y_1, y_2, y_3, y_4\}$ 表示 $A$ 四种结果发生的次数。现要求根据试验的结果求出事件 $A$ 所属分布的参数 $\theta$ 的 MLE。事件 $A$ 的总体分布为多项分布，故其似然函数为：
$$\begin{aligned}
    \mathrm{L}(\theta;y) &\propto (\frac{2-\theta}{4})^{y_1}(\frac{1-\theta}{4})^{y_2}(\frac{1+\theta}{4})^{y_3}(\frac{\theta}{4})^{y_4} \\
                         &\propto (2-\theta)^{y_1}(1-\theta)^{y_2}(1+\theta)^{y_3}\theta^{y_4}
\end{aligned}\tag{2}
$$

直接求解上式是非常复杂的，现引入变量 $\{z_1, z_2\}$ 来降维，以简化求解过程。

1. 从第一种结果中分出一部分结果为 $z_1$，其概率为 $\frac{1-\theta}{4}$。第一种结果中剩余的结果计数为 $y_1-z_1$，其概率为 $\frac{1}{4}$。
2. 从第三种结果中分出一部分结果为 $z_2$，其概率为 $\frac{\theta}{4}$。第三种结果中剩余的结果计数为 $y_3-z_2$，其概率为 $\frac{1}{4}$。

由于 $\{z_1,z_2\}$ 是人为引入以简化计算的，因此其不可观测，因此也称之为**潜变量**（Latent variable）或**不完全数据**，同时称数据 $(y,z)$ 为**完全数据**（Complete data）。基于完全数据的似然函数为：
$$\begin{aligned}
    \mathrm{L}(\theta;y,z) & \propto (\frac{1}{4})^{y_1-z_1}(\frac{1-\theta}{4})^{y_2+z_1}(\frac{1}{4})^{y_3-z_2}(\frac{\theta}{4})^{y_4+z_2} \\
                           & \propto (1-\theta)^{y_2+z_1}\theta^{y_4+z_2}
\end{aligned}\tag{3}$$

其对数似然函数为：
$$l(\theta;y,z)=(y_2+z_1)\ln(1-\theta)+(y_4+z_2)\ln\theta\tag{4}$$

如果 $(y,z)$ 均已知，则由上式可以很容易得到 $\theta$ 的无偏估计。但现实是 $\{z_1, z_2\}$ 是人为定义的，因此无法得知其值。但是这两个变量有如下性质：
$$z_1 \sim b\left(y_1, \frac{1-\theta}{2-\theta}\right),\quad z_2 \sim b\left(y_3, \frac{\theta}{1+\theta}\right)\tag{5}$$

EM 算法是指以下述的方式分两步进行迭代求解。

> **E 步**：在已有观测数据 $y$ 及第 $i$ 步估计值 $\theta=\theta^{(i)}$ 的情况下，求基于完全数据的对数似然函数的期望。根据式 $(5)$ 可得 $\{z_1,z_2\}$ 的分布及期望，因此该步即是把式 $(4)$ 中与 $z$ 有关的部分积分掉。
> $$\mathrm{Q}(\theta|y,\theta^{(i)})=\mathrm{E}_zl(\theta;y,z)$$
> **M 步**：求 $\mathrm{Q}(\theta|y,\theta^{(i)})$ 关于 $\theta$ 的最大值 $\theta^{(i+1)}$，即找到 $\theta^{(i+1)}$，使得正式成立：
> $$\mathrm{Q}(\theta^{(i+1)}|y,\theta^{(i)})=\max_\theta\mathrm{Q}(\theta|y,\theta^{(i)})$$

这样就得到了从 $\theta^{(i)}$ 到 $\theta^{(i+1)}$ 的一次迭代。重复上述过程直到收敛，即得到 $\theta$ 的 MLE 估计。

## 四、最小方差无偏估计

相合性和渐近正态性是在大样本场合下评价估计的两个重要标准。在样本量较小时，使用一些基于小样本的评价标准：对无偏估计使用方差，对有偏估计使用均方误差。

### 4.1 均方误差 Mean squared error

评价一个点估计的好坏使用的度量指标通常是点估计值 $\hat{\theta}$ 与参数直值 $\theta$ 之间距离的函数。最常用的函数是距离的平方。

> **定义：均方误差 Mean squared error**
> 均方误差是指点估记值 $\hat{\theta}$ 与真值 $\theta$ 之间的**距离**的平方的期望。
> $$\mathrm{MSE}(\hat{\theta}) = \mathrm{E}(\hat{\theta} - \theta)^2$$

注意到（$\mu = \mathrm{E}(\hat{\theta})$）：

$$
\begin{aligned}
    \mathrm{MSE}(\hat{\theta}) &= \mathrm{E} \left[(\hat{\theta}-\mu)+(\mu-\theta)\right]^2 \\
                             &= \mathrm{E}(\hat{\theta}-\mu)^2 + (\mu-\theta)^2 + 2\mathrm{E}\left[(\hat{\theta}-\mu)(\mu-\theta) \right] \\
                             &= \mathrm{Var}(\hat{\theta}) + (\mu-\theta)
\end{aligned}$$

均方误差由点估计的方差和偏差 $|\mathrm{E}(\hat{\theta})-\theta|$ 的平方两部分组成。如果 $\hat{\theta}$ 是 $\theta$ 的无偏估计，则 $\mathrm{MSE}(\hat{\theta})=\mathrm{Var}(\hat{\theta})$。

> **定义：一致最小均方误差估计 Minimum mean squared error estimator**
> 设有样本 $\{x_1, x_2, \dots, x_n\}$ 对待估参数 $\theta$，有一个估计类。如果该估计类中存在 $\hat{\theta}(x_1, x_2, \dots, x_n)$，对该估计类中另外任意一个 $\theta$ 的估计 $\tilde{\theta}$，在参数空间 $\Theta$ 上都有 $(1)$ 式，则称 $\hat{\theta}$ 是该估计类中 $\theta$ 的一致最小均方误差估计。
> $$\mathrm{MSE}_\theta(\hat{\theta}) \leq \mathrm{MSE}_\theta(\tilde{\theta})$$

## 五、贝叶斯估计

## 六、区间估计

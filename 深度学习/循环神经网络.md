<!-- @import "../../引用/my-style.less" -->

# 循环神经网络 Recurrent neural network

神经网络是一种节点定向连接成环的人工神经网络，其内部状态可以展示动态时序行为。不同于前馈神经网络的是，RNN可以利用它内部的记忆来处理任意时序的输入序列，因此可以更容易地处理序列问题，如不分段的手写识别、语音识别等。

- `state`：神经元内部的状态

- `hidden state`：`LSTM`神经元中特有的隐藏状态

RNN 的一般工作过程如下：

```dot
digraph g1{
    compound=true
    graph [rankdir = "LR"];
    node [fontsize = "16", shape = "plaintext"];
    edge [];

    a [label="s[t-1]"]
    b [label="s[t]"]
    x [label="x[t] (输入)"]
    y [label="y[t] (输出)"]
    cell [label="RNN cell", shape=box]

    a -> cell -> b
    x -> cell -> y

    {rank=same; a cell b}
}
```

- `s[t-1]` 与 `s[t]` 对应网络内部的 `state` 计算前后的两个不同状态。

## Vanilla RNN

单层的 Vanilla RNN/GRU Cell 只有一个循环部件 `state`，其工作过程如下：

```dot
digraph vanilla {
    compound=true
    graph [rankdir = "LR"];
    node [fontsize = "16", shape = "plaintext"];
    edge [];

    h [label="s[t]"]
    x [label="x[t]"]

    subgraph cluster0 {
        label="single layer \nvanilla RNN/GRU cell"
        a [label="s[t-1]"]
        b [label="s[t]"]
        compute_cell [label="计算\n单元", shape=box]
        a -> compute_cell
        compute_cell -> b
        {rank=same; compute_cell b a}
    }

    x -> compute_cell -> h
}
```

- `s[t]` 是 cell 的内部部件 `state`，`s_t = y_t = h_t`。

构筑一个多层的 Vanilla RNN/GRU Cell，可以将多层 `cell` 抽象成一个整体，当成一层大的 `cell`，原先各层之间的关系都当成这个大的 `cell` 的内部计算过程或数据流动过程。因此对外而言，多层的 RNN 和单层的 RNN 的接口就是一模一样的。多层 RNN 只是一个内部计算更复杂的单层 RNN。

```dot
digraph vanilla {
    compound=true
    graph [rankdir = "LR"];
    node [fontsize = "16", shape = "plaintext"];
    edge [];

    out [label="s[t][n]"]
    x [label="x[t]"]

    subgraph cluster0 {
        label="multi-layer \nvanilla RNN/GRU cell"
        compute_cell1 [label="计算\n单元", shape=box]
        compute_cell2 [label="计算\n单元", shape=box]
        compute_cell3 [label="计算\n单元", shape=box]
        compute_celln [label="计算\n单元", shape=box]

        compute_cell1 -> compute_cell2
        compute_cell2 -> compute_cell3
        compute_cell3 -> compute_celln
        {rank=same; compute_cell1, compute_cell2, compute_cell3, compute_celln, x, out}

    }

    subgraph cluster01 {
        label="s[t-1]"
        a1 [label="s[t-1][1]"]
        a2 [label="s[t-1][2]"]
        a3 [label="s[t-1][...]"]
        an [label="s[t-1][n]"]
    }

    subgraph cluster02 {
        label="s[t]"
        b1 [label="s[t][1]"]
        b2 [label="s[t][2]"]
        b3 [label="s[t][...]"]
        bn [label="s[t][n]"]
    }


        a1 -> compute_cell1 -> b1
        a2 -> compute_cell2 -> b2
        a3 -> compute_cell3 -> b3
        an -> compute_celln -> bn


    x -> compute_cell1
    compute_celln -> out
}
```

        {rank=same; compute_cell1 a1 b1}
        {rank=same; compute_cell2 a2 b2}
        {rank=same; compute_cell3 a3 b3}
        {rank=same; compute_celln an bn}

## Long short-term memory (LSTM)

Long short-term memory 单元是 RNN 单元的一种，由这种单元构成的网络叫做 **LSTM network**。

`LSTM` 单元的循环部件有两部分，输出层只利用 hidden state 的信息，而不直接利用 cell:

- 内部 cell 的值
- 根据 cell 和 `output gate` 计算出的 `hidden state`。

单层 `LSTM` 单元的工作过程：

```dot
digraph lstm_single {
    compound=true
    graph [rankdir = "LR"];
    node [fontsize = "16", shape = "plaintext"];
    edge [];

    x [label="x[t]"]
    out [label="h[t]"]

    subgraph cluster0 {
        state_t_nega_1 [label="c[t-1]\nh[t-1]", shape=box]
        state_t [label="c[t]\nh[t]", shape=box]
        compute_cell [label="计算\n单元", shape=box]

        state_t_nega_1 -> compute_cell [label="s[t-1]",style=dotted]
        compute_cell -> state_t [label="s[t]", style=dotted]

        {rank=same; state_t_nega_1 compute_cell state_t}
    }

    x -> compute_cell -> out
}
```

其中真正用于循环的状态 $s_t$ 其实是 $(c_t, h_t)$ 组成的 `tuple`，就是 TensorFlow 里的 `LSTMStateTuple`。

`LSTM` 单元的输出 $y_t$ 仅仅是 $h_t$。网络后面再接一个全连接层然后用 softmax 做分类，这个全连接层的输入仅仅是 $h_t$，而没有 $c_t$。这与 vanilla RNN 的输出完全不同。
# 反向传播算法 Backpropagation algorithm

反向传播算法（BP算法）是一种计算用于优化神经网络参数的梯度的算法。算法由信号的正向传播和误差的反向传播两个过程组成。

- **训练集**：

$$D = \{(x_1, y_1), (x_2, y_2), \dots, (x_m, y_m)\}$$

$$x \in R^d$$

$$y \in R^C, y_0 = +1$$

- **突触权值**：

$$w \in R, w_0 = b_j$$

- **激活函数**：$\varphi(\cdot)$

  - 线性函数
  - 硬限幅器
  - 分段线性函数
  - sigmoid 函数
    - logistic 函数
    - 双曲正切函数

- **学习率**：$\eta$

- **阈值**：$\theta$，用于将数值分割成具体的类

- 隐藏层的一个神经元 $j$

- 神经元 $k$ 是输出节点

神经元 $j$ 的简单构造有：

$$v = \sum_{i=1}^mw_ix_i + b$$

$$y = \varphi(v)$$

## 前向阶段

正向传播时，输入样本从输入层进入网络，经隐层逐层传递至输出层，如果输出层的实际输出与期望输出不同，则转至误差反向传播；如果输出层的实际输出与期望输出相同，结束学习算法。

## 反向阶段

反向传播时，将输出误差(期望输出与实际输出之差)按原通路反传计算，通过隐层反向，直至输入层，在反传过程中将误差分摊给各层的各个单元，获得各层各单元的误差信号，并将其作为修正各单元权值的根据。

一个多变量的函数的偏导数，就是它关于其中一个变量的导数而保持其他变量恒定（相对于全导数，在其中所有变量都允许变化）。

$$\Delta w_{ji}(n)= \eta \delta_j(n)y_i(n)$$

### 输出层

对于样本 $(x, y)$，神经网络的输出为 $\hat{y} = (\hat{y}_1, \hat{y}_2,\dots , \hat{y}_k, \dots, \hat{y}_l)$。其中：

$$\hat{y}_j = \varphi(v_k)$$

计算神经网络的预测值 $\hat{y}$ 与真值 $y$ 之间的误差 $e = \hat{y} - y$。因此均方误差为：

$$MSE(\hat{y})= \mathcal{E} = \frac12\sum_{e \in C}e_k^2$$

BP算法基于梯度下降（Gradient descent）的优化策略，以目标的负梯度为权重调整方向，使误差值减小到最低限度。

$$w = w + \Delta_w, \quad \Delta_w = -\eta \frac {\partial_{\mathcal E}} {\partial_w}$$

误差值 $\mathcal{E}_k$ 是预测值 $\hat{y}$ 的函数，而预测值 $\hat{y}$ 是权重 $w$ 的函数。利用链式法则，可得：

权值和阈值不断调整的过程，就是网络的学习与训练过程，经过信号正向传播与误差反向传播，权值和阈值的调整反复进行，一直进行到预先设定的学习训练次数，或输出误差减小到允许的程度。

## 二次代价函数

$$C = \frac 1 {2n} ||y(x) - a^L(x)||^2$$

- $C$ 表示代价
- $x$ 表示样本
- $y$ 表示实际值
- $a$ 表示输出值
- $n$ 表示样本的总数

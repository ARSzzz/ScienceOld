# 信息论 Information theory

香农[^香农]在进行信息的定量计算的时候，明确地把信息量定义为：**随机不定性程度的减少**。

## 一、信息量度量 Measure of information

一份数据的信息量大小和它的不确定性有直接的关系，如 $\{1, 2, 3, 4, 5\}$ 比 $\{1, 1, 1, 1, 1\}$ 的信息量要大。将一份数据中每个元组的取值作为一个随机变量，用熵来量化该随机变量的不确定性，以量化其信息量。

### 1.1 熵 Entropy

> **定义：香农熵 Shannon entropy**
> 设 $X$为一个离散随机变量，它的可能取值为 $\{x_1, x_2, \dots, x_n\}$，它的概率质量函数是 $\mathrm{P}(X)$。定义它的香农熵（简称熵）如下式 $(1)$。
> 其中，$\mathrm{E}(\cdot)$ 是数学期望函数，$\mathrm{I}(\cdot)$ 是 `information content`。$\mathrm{I}(X)$ 本身也是一个随机变量。香农熵也可以写成式 $(2)$：
> $$\Eta(X)=\mathrm{E}[\mathrm{I}(X)] = \mathrm{E}[-\log(\mathrm{P}(X))]\tag{1}$$
>
> $$\Eta(X)=\sum_{i=1}^n\mathrm{P}(x_i)\mathrm{I}(x_i) = -\sum_{i=1}^n\mathrm{P}(x_i)\log_b\mathrm{P}(x_i)\tag{2}$$

上式中 $\log$ 的底数 $b$ 一般取 $\{2, \mathrm{e}, 10\}$。因为在计算机中数据以二进制存储，所以一般取 $b=2$。
随机变量 $X$ 的不确定性越大，则它的熵的值也就越大。

### 1.2 基尼不纯度 gini impurity

基尼不纯度是另一种用于量化一份数据中信息量的方法，它是指：将来自集合中的某种结果随机应用于集合中某一数据项的预期误差率。如何集合中的每一个数据项都属于同一分类，那么推测的结果总会是正确的，因此误差率是 0；如果有 4 种可能的结果均匀分布在集合内，出错可能性是75%，基尼不纯度为 0.75。

> **定义：基尼不纯度 Gini impurity**
> 设 $X$为一个离散随机变量，它的可能取值为 $\{x_1, x_2, \dots, x_n\}$，它的概率质量函数是 $P(X)$。$X$ 的基尼不纯度定义如下：
> $$\begin{aligned}
> \overbrace{\mathrm{I}_G(D)}^{\text{Gini impurity}}
> &= \sum_{i=1}^n[\mathrm{P}(x_i)\sum_{j\neq i}\mathrm{P}(x_j)]\\
> &= \sum_{i=1}^n[\mathrm{P}(x_i)(1-\mathrm{P}(x_i))] \\
> &= 1-\sum_{i=1}^n\mathrm{P}(x_i)^2
> \end{aligned}$$

## 二、信息增加度量 Measure of information increase

> **定义：`Information gain`**
> 假设按某属性 $A=\{a_1,a_2,\cdots,a_v\}$ 来划分训练集 $D$ 中的元组，得到相对应的 $v$ 个子集 $\{D_1,D_2,⋯,D_v\}$。使用**子集的信息熵（纯度）的期望**来度量划分后的 $D$ 的总体信息量。
> $$\begin{aligned}
\Eta(D|A) &=\sum_{i=1}^v p(a_i) \cdot \Eta(D_i)
\end{aligned}$$
>
> 将训练集 $D$ 按属性 $A$ 划分前后的两个熵相减，称该差值为：按属性 $A$ 进行划分时的 `Information gain`。
> $$\overbrace{IG(D,A)}^{\text{Information gain}} =
> \overbrace{H(D)}^{\text{Entropy of D}} - \overbrace{H(D|A)}^{\text{Weighted sum of entropy}}$$

使用 `Information gain` 作为属性选择度量有其局限性，其偏向具有许多输出的测试，即倾向于选择具有取值的属性。这种度量使划分后的子集只包含很少的元组，因此每一个子集更容易是纯的。

相对于ID3，C4.5使用 `Information gain ratio` 来对 `Information gain` 进行修正。

> **定义：信息增益率 Information gain ratio**
> $$\overbrace{IV(D,A)}^{\text{Intrinsic value}} = -\sum_{i=1}^v p(a_i) \cdot \log_2p(a_i)\\
\underbrace{IGR(A)}_{\text{Information gain ratio}}=
\frac {IG(D,A)} {IV(D,A)}$$

- 决策树的 CART（Classification and regression tree）算法使用基尼不纯度来选取分裂属性。

[^香农]: 克劳德·艾尔伍德·香农（Claude Elwood Shannon, 1916/4/30-2001/2/24），是美国数学家、信息论的创始人。
